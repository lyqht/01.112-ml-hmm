{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.112 Machine Learning Design Project\n",
    "\n",
    "## About the Project\n",
    "\n",
    "We have 4 datasets in the `/data` folder. For each dataset, there is: \n",
    "- a labelled training set train, \n",
    "- an unlabelled development set `dev.in`\n",
    "- a labelled development set `dev.out` \n",
    "\n",
    "The labelled data has the format of: `token` `\\t` `tag`\n",
    "- one token per line\n",
    "- token and tag separated by tab \n",
    "- single empty lines that separates sentences\n",
    "\n",
    "For the labels, they are slightly different for different datasets.\n",
    "- SG, CN (Entity):\n",
    "    - B-*: Beginning of entity\n",
    "    - I-*: Inside of entity\n",
    "    - O: Outside of any entity\n",
    "- EN, AL (Phrase):\n",
    "    - B-VP: Beginning of Verb Phrase\n",
    "    - I-VP: Inside of Verb Phrase\n",
    "    - *-NP: Noun Phrase\n",
    "    - *PP: Propositional Phrase\n",
    "    - O: Outside of any phrase\n",
    "\n",
    "*Goal*: Build sequence labelling systems from training data (x) and use it to predict tag sequences for new sentences (y).\n",
    "\n",
    "## Team members \n",
    "- Andri Setiawan Susanto\n",
    "- Eldon Lim \n",
    "- Tey Siew Wen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Already completed individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Write a function that estimates the emission parameters from the training set using MLE (maximum likelihood estimation):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\n",
    "\n",
    "1. Make a modified training set by replacing those words that appear $<k$ times in the training set with a special word token `#UNK#` before training.\n",
    "2. During testing phase, ifaworddoesnot appear in the modified training set, we also replace that wordwith `#UNK#`.\n",
    "3. Compute Emission Paramters with the function in (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the four datasets EN, AL, CN, and SG, learn these parameters with `train`, and evaluate your\n",
    "system on the development set `dev.in` for each of the dataset. Write your output to `dev.p2.out`\n",
    "for the four datasets respectively. Compare your outputs and the gold-standard outputs in `dev.out`\n",
    "and report the precision, recall and F scores of such a baseline system for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def emissionPara(arr, k, replaceWord):\n",
    "    \"\"\"\n",
    "    calculates the emission probabilities given a numpy array containing arrays of [(x,y)]\n",
    "    \n",
    "    returns:\n",
    "    emission_dict => a dictionary in the form of {(x,y): probability}\n",
    "    valid_x => list of x that has occurences more or equal to k times.\n",
    "    xy_dic => a dictionary in the form of {x: y} where y is the most probable label given x\n",
    "    \"\"\"\n",
    "    x_counter = defaultdict(int)\n",
    "    y_counter = defaultdict(int)\n",
    "    xy_counter = defaultdict(int)\n",
    "    x_labels = defaultdict(list)\n",
    "    emission_params = {}\n",
    "    xy_dict = {}\n",
    "\n",
    "    obs = 0\n",
    "    for x_y in arr:\n",
    "        x, y = x_y[0].split(\" \")\n",
    "        obs += 1\n",
    "        x_counter[x] += 1\n",
    "        y_counter[y] += 1\n",
    "        xy_counter[x,y] += 1\n",
    "        if y not in x_labels[x]:\n",
    "            x_labels[x].append(y)\n",
    "        \n",
    "    x_to_replace = [x for x in x_counter if x_counter[x] < k]\n",
    "    print(f\"There are {obs} observations, {len(x_to_replace)} values of x is to be replaced \")\n",
    "\n",
    "    # removing x occurences less than k times from x_counter and its labels\n",
    "    for r in x_to_replace:\n",
    "        for label in x_labels[r]:\n",
    "            if label not in x_labels[replaceWord]:\n",
    "                x_labels[replaceWord].append(label)\n",
    "            xy_counter[replaceWord, label] += xy_counter[r, label]\n",
    "            \n",
    "            del xy_counter[r,label]\n",
    "        del x_counter[r]\n",
    "        del x_labels[r]\n",
    "        \n",
    "    # calculation of emission probabilities        \n",
    "    for x_y, x_y_count in xy_counter.items():\n",
    "        y = x_y[1]\n",
    "        emission_params[x_y] = x_y_count / y_counter[y]\n",
    "\n",
    "    # get best labels\n",
    "    for x, labels in x_labels.items():\n",
    "        emission_probs = np.zeros(len(labels))\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            emission_probs[i] = emission_params[x,label]\n",
    "        xy_dict[x] = labels[np.argmax(emission_probs)]\n",
    "    return emission_params, x_counter.keys(), xy_dict\n",
    "\n",
    "def predict(data, xy_dict, replaceWord):\n",
    "    print(\"Predicting labels\")\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    def replace_string(x):\n",
    "        if x not in xy_dict:\n",
    "            return \"{} {}\".format(replaceWord, xy_dict[replaceWord])\n",
    "        else:\n",
    "            return \"{} {}\".format(x, xy_dict[x])\n",
    "         \n",
    "    return data[\"x\"].apply(lambda s: replace_string(s) if str(s) != \"nan\" else \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that estimates the transition parameters from the training set using MLE (maximum likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_columns(df_column):\n",
    "    new = df_column.str.split(\" \", n=1, expand=True)\n",
    "    return new[0], new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def transitionPara(data):\n",
    "    y_count = defaultdict(int)\n",
    "    subseq_count = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(data) - 1):\n",
    "        curr_xy = data[i][0]\n",
    "        next_xy = data[i+1][0]\n",
    "        \n",
    "        if i == 0:\n",
    "            x, y = curr_xy.split(\" \")\n",
    "            subseq_count[\"START\", y] += 1\n",
    "            y_count[y] += 1\n",
    "            y_count[\"START\"] += 1\n",
    "            continue\n",
    "        \n",
    "        elif curr_xy == None or curr_xy == np.nan or str(curr_xy) == \"nan\":\n",
    "            x, y = next_xy.split(\" \")\n",
    "            subseq_count[\"START\", y] += 1\n",
    "            y_count[y] +=1\n",
    "            y_count[\"START\"] += 1\n",
    "        \n",
    "        elif i+1 == len(data) or next_xy == None or next_xy == np.nan or str(next_xy) == \"nan\":\n",
    "            x, y = curr_xy.split(\" \")\n",
    "            subseq_count[y,\"END\"] +=1\n",
    "            y_count[y] += 1\n",
    "            y_count[\"END\"] += 1\n",
    "            \n",
    "        else:\n",
    "            y1 = curr_xy.split()[1]\n",
    "            y2 = next_xy.split()[1]\n",
    "            subseq_count[y1,y2] +=1\n",
    "            y_count[y1] += 1\n",
    "        \n",
    "        # Calculation of transition params\n",
    "    transition_dict = {}\n",
    "\n",
    "    for k,v in subseq_count.items():\n",
    "        y1 = k[0]\n",
    "        y2 = k[1]\n",
    "        transition_dict[y1,y2] = subseq_count[y1,y2] / y_count[y1]\n",
    "    \n",
    "    del y_count[\"END\"]\n",
    "    del y_count[\"START\"]\n",
    "\n",
    "    return transition_dict, subseq_count, y_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(unique_word_list):\n",
    "    #This is for the starting for viterbi\n",
    "    global nodes\n",
    "    \n",
    "    num_nodes_per_col = len(nodes)\n",
    "    store=np.zeros(num_nodes_per_col)   #store = the storage for scores for all the nodes. \n",
    "    scorelist=np.zeros((num_nodes_per_col, len(unique_word_list) + 1))\n",
    "    rankings = np.zeros((num_nodes_per_col, len(unique_word_list) + 1))\n",
    "    \n",
    "    for i in range(num_nodes_per_col):\n",
    "        emission_score = emission(nodes[i],unique_word_list[0])\n",
    "        transition_score = transition(\"START\",nodes[i])\n",
    "        if transition_score == 0 or emission_score == 0:\n",
    "            store[i] = -1e100\n",
    "        else:\n",
    "            store[i] = np.log(emission_score)+np.log(transition_score)\n",
    "            \n",
    "    scorelist[:,0] = store\n",
    "    store = np.zeros(num_nodes_per_col)\n",
    "    score_per_node=np.zeros(num_nodes_per_col)\n",
    "    \n",
    "    #This is for the middle portion for viterbi\n",
    "    #score per node = prevnode*emission*transition\n",
    "\n",
    "    if len(unique_word_list)>1:\n",
    "        for j in range(len(unique_word_list)-1): #for the whole length in sentence\n",
    "            for k in range(num_nodes_per_col): #for each node\n",
    "                for l in range(num_nodes_per_col): #for 1 node, transition from prev node to current node\n",
    "                    prev_node = scorelist[l][j]\n",
    "                    emission_score = emission(nodes[k],unique_word_list[j+1])\n",
    "                    transition_score = transition(nodes[l],nodes[k])\n",
    "                    \n",
    "                    if transition_score == 0 or emission_score == 0:\n",
    "                        score_per_node[l] = np.NINF\n",
    "                    else:   \n",
    "                        score_per_node[l] = prev_node+np.log(emission_score)+np.log(transition_score) \n",
    "                \n",
    "                store[k] = np.max(score_per_node) # max path\n",
    "                score_per_node=np.zeros(num_nodes_per_col)\n",
    "            \n",
    "            scorelist[:,j+1] = store\n",
    "            store = np.zeros(num_nodes_per_col)\n",
    "                      \n",
    "        score_at_stop=np.zeros(num_nodes_per_col)\n",
    "        \n",
    "        #This is for the STOP for viterbi\n",
    "        for m in range(num_nodes_per_col):\n",
    "            score_at_stop[m] = np.log(transition(nodes[m],\"END\")) + scorelist[m][len(unique_word_list)-1]\n",
    "        scorelist[:,-1] = np.full(num_nodes_per_col,np.max(score_at_stop))\n",
    "        \n",
    "    return scorelist\n",
    "  \n",
    "def viterbi_backtrack(scorelist):\n",
    "    \"\"\"\n",
    "    back tracking for viterbi\n",
    "    node value*transition = array, then find max, then find position. use position for next step.\n",
    "    np.argmax returns index of max in the element.\n",
    "    The final score on the score list is for end\n",
    "    \"\"\" \n",
    "    global nodes\n",
    "    \n",
    "    scorelist = np.flip(scorelist,axis=1) #reverse the score list so easier to calculate.\n",
    "    \n",
    "    max_node_index = 0 \n",
    "    num_obs = scorelist.shape[1]\n",
    "    num_nodes = scorelist.shape[0]\n",
    "    node_holder = np.zeros(num_nodes)\n",
    "    path = []\n",
    "\n",
    "    if (num_obs == 1):\n",
    "        for k in range (num_nodes):\n",
    "            calculate_max_node = scorelist[0][k] + np.log(transition(nodes[k],\"END\"))\n",
    "            node_holder[i] = calculate_max_node\n",
    "        path.append(nodes[np.argmax(node_holder)])\n",
    "        return(path[::-1])\n",
    "\n",
    "    for i in range (1,num_obs): # for length of sentence\n",
    "        for j in range(num_nodes): #for each node\n",
    "            if (i==1):\n",
    "                calculate_max_node = scorelist[j][i] + np.log(transition(nodes[j],\"END\"))\n",
    "                node_holder[j] = calculate_max_node\n",
    "            else:\n",
    "                calculate_max_node = scorelist[j][i] + np.log(transition(nodes[j],nodes[max_node_index]))\n",
    "                node_holder[j] = calculate_max_node\n",
    "        \n",
    "        max_node_index=np.argmax(node_holder)\n",
    "        path.append(nodes[np.argmax(node_holder)])\n",
    "        node_holder=np.zeros(num_nodes)\n",
    "\n",
    "    return(path[::-1])\n",
    "\n",
    "def emission(node,word):\n",
    "    global emission_dict\n",
    "    global nodes\n",
    "    pair = word,node\n",
    "    detector = 0 # this is used to find if word exist in the dictionary\n",
    "    if pair not in emission_dict.keys(): #if the combination cannot be found in the dictionary\n",
    "                                         #Either the word exists, or word is new. \n",
    "        for o in nodes:\n",
    "            missing_pair = word,o\n",
    "            if missing_pair in emission_dict.keys(): #\n",
    "                detector = 1 # to detect if word exist in dictionary.\n",
    "                break\n",
    "        if detector == 1:\n",
    "            score=0   #this means that this node is not the correct node.\n",
    "        else:\n",
    "            replaced_text = \"#UNK#\",node\n",
    "            if replaced_text in emission_dict.keys():\n",
    "                score = emission_dict[replaced_text] #if label have #unk#\n",
    "                \n",
    "            else:\n",
    "                score = 0   #if label does not have #unk#, then set to 0.\n",
    "    else:\n",
    "        score = emission_dict[pair]\n",
    "    return score\n",
    "\n",
    "\n",
    "def transition(x1,x2):\n",
    "    global transition_dic\n",
    "    #will use this to search the transition from x1 to x2\n",
    "    pair = x1,x2\n",
    "    if pair not in transition_dic.keys():\n",
    "        score = 0\n",
    "    else:\n",
    "        score = transition_dic[x1,x2]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq \n",
    "\n",
    "def kViterbiParallel(nodes, obs, num_k):\n",
    "    global transition\n",
    "    global emission\n",
    "    \n",
    "    \"\"\"\n",
    "    scores = highest probability of any path that reaches point i\n",
    "    phi(i, k) = kth lowest cost to reach node i at col t from node 1 at col 0.\n",
    "    rank = The ranking of multiple paths through a state\n",
    "    \"\"\"\n",
    "    num_nodes = len(nodes)\n",
    "    num_words = len(obs)\n",
    "\n",
    "    scores = np.zeros((num_words+1, num_nodes, num_k),float)\n",
    "    phi = np.zeros((num_words+1, num_nodes, num_k),int)\n",
    "    rank = np.zeros((num_words+1, num_nodes, num_k),int)\n",
    "\n",
    "    # for k in range(K):\n",
    "    for i in range(num_nodes):\n",
    "        curr_emission = emission(nodes[i], obs[0])\n",
    "        curr_transition = transition(\"START\", nodes[i])\n",
    "        scores[0, i, 0] = np.log(curr_emission) + np.log(curr_transition)\n",
    "        phi[0, i, 0] = i\n",
    "\n",
    "        # Set the other options  to 0 initially\n",
    "        for k in range(1, num_k):\n",
    "            scores[0, i, k] = 0.0\n",
    "            phi[0, i, k] = i\n",
    "\n",
    "    # Go forward calculating top k scoring paths\n",
    "    # for each state s1 from previous state s2 at time step t\n",
    "    for t in range(1, num_words-1):\n",
    "        for s1 in range(num_nodes):\n",
    "          \n",
    "            h = []  # heap structure for priority queue \n",
    "          \n",
    "            for s2 in range(num_nodes):\n",
    "                for k in range(num_k):\n",
    "                    curr_emission = emission(nodes[s1], obs[t])\n",
    "                    curr_transition = transition(nodes[s1], nodes[s2])\n",
    "                    prob = scores[t - 1, s2, k] + np.log(curr_transition) + np.log(curr_emission)\n",
    "\n",
    "                    state = s2\n",
    "\n",
    "                    # Push the probability and state that led to it\n",
    "                    heapq.heappush(h, (prob, s2))\n",
    "\n",
    "            #Get the sorted list\n",
    "            h_sorted = [heapq.heappop(h) for i in range(len(h))]\n",
    "            h_sorted.reverse()\n",
    "\n",
    "            #We need to keep a ranking if a path crosses a state more than once\n",
    "            rankDict = dict()\n",
    "\n",
    "            #Retain the top k scoring paths and their phi and rankings\n",
    "            for k in range(0, num_k):\n",
    "                scores[t, s1, k] = h_sorted[k][0]\n",
    "                phi[t, s1, k] = h_sorted[k][1]\n",
    "\n",
    "                state = h_sorted[k][1]\n",
    "\n",
    "                if state in rankDict:\n",
    "                    rankDict[state] = rankDict[state] + 1\n",
    "                else:\n",
    "                    rankDict[state] = 0\n",
    "\n",
    "                rank[t, s1, k] = rankDict[state]\n",
    "                \n",
    "    \n",
    "    # for END transition prob calculation\n",
    "    h = []\n",
    "    rankDict = dict()\n",
    "    for i in range(num_nodes):\n",
    "        curr_emission = emission(nodes[i], obs[-1])\n",
    "        curr_transition = transition(nodes[i], \"END\")\n",
    "        prob = scores[num_words, i, k] + np.log(curr_transition) + np.log(curr_emission)\n",
    "        heapq.heappush(h, (prob, i))\n",
    "    h_sorted = [heapq.heappop(h) for i in range(len(h))]\n",
    "    h_sorted.reverse()\n",
    "    for k in range(0, num_k):\n",
    "        scores[num_words,i , k] = h_sorted[k][0]\n",
    "        phi[num_words, i, k] = h_sorted[k][1]\n",
    "        state = h_sorted[k][1]\n",
    "\n",
    "        if state in rankDict:\n",
    "            rankDict[state] = rankDict[state] + 1\n",
    "        else:\n",
    "            rankDict[state] = 0\n",
    "\n",
    "        rank[num_words, i, k] = rankDict[state]\n",
    "        \n",
    "    # Put all the last items on the stack\n",
    "    h = []\n",
    "\n",
    "    # Get all the num_k from all the states\n",
    "    for s1 in range(num_nodes):\n",
    "        for k in range(num_k):\n",
    "            prob = scores[num_words, s1, k]\n",
    "\n",
    "            #Sort by the probability, but retain what state it came from and the k\n",
    "            heapq.heappush(h, (prob, s1, k))\n",
    "\n",
    "    # Then get sorted by the probability including its state and num_k\n",
    "    h_sorted = [heapq.heappop(h) for i in range(len(h))]\n",
    "    h_sorted.reverse()\n",
    "\n",
    "    # init blank path\n",
    "    path = np.zeros((num_k, num_words), int)\n",
    "    path_probs = np.zeros((num_k, num_words),float)\n",
    "\n",
    "    # Backpropagation\n",
    "    for k in range(num_k):\n",
    "        #The maximum probability and the state it came from\n",
    "        max_prob = h_sorted[k][0]\n",
    "        state = h_sorted[k][1]\n",
    "        rankK = h_sorted[k][2]\n",
    "\n",
    "        # Assign to output arrays\n",
    "        path_probs[k][-1] = max_prob\n",
    "        path[k][-1] = state\n",
    "\n",
    "        #Then from t down to 0 store the correct sequence for t+1\n",
    "        for t in range(num_words - 2, -1, -1):\n",
    "            #The next state and its rank\n",
    "            nextState = path[k][t+1]\n",
    "\n",
    "            #Get the new state\n",
    "            p = phi[t+1][nextState][rankK]\n",
    "\n",
    "            #Pop into output array\n",
    "            path[k][t] = p\n",
    "\n",
    "            #Get the correct ranking for the next phi\n",
    "            rankK = rank[t + 1][nextState][rankK]\n",
    "\n",
    "    return path, path_probs, scores, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_blank_row(data):\n",
    "    start = time.process_time()   \n",
    "    \n",
    "    df= pd.read_csv(data, sep='/n', delimiter=None, names=['original'],index_col=False,engine=\"python\",skip_blank_lines=False)\n",
    "    # dropping null value columns to avoid errors \n",
    "    \n",
    "    # new data frame with split value columns \n",
    "    df[\"x\"], df[\"y\"] = split_into_columns(df[\"original\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceList(data):\n",
    "    lines=[]\n",
    "    line=[]\n",
    "    x= data\n",
    "    for label in x['x']:\n",
    "        if pd.isnull(label)==False:\n",
    "            line.append(label)\n",
    "        else:\n",
    "            lines.append(line)\n",
    "            line = []\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalresult(sequence_log,predata_blank):\n",
    "    dataframe = []\n",
    "    count=0\n",
    "    for i in range(len(sequence_log)):\n",
    "        for text in sequence_log[i]:\n",
    "            dataframe.append(text)\n",
    "            count+=1\n",
    "        dataframe.append(\"\")\n",
    "    dftest=pd.DataFrame(dataframe)\n",
    "    final = pd.DataFrame()\n",
    "    final['result'] = predata_blank['x'] + \" \" +dftest[0]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing sentiment analysis for data folder  CN\n",
      "There are 168751 observations, 16402 values of x is to be replaced \n",
      "Predicting labels\n",
      "0        一 I-negative\n",
      "1    #UNK# B-negative\n",
      "2    #UNK# B-negative\n",
      "Name: x, dtype: object\n",
      "Writing the final result to dev.p2.out...\n",
      "Finished writing\n",
      "Performing Viterbi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/estee/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Viterbi and Backtrack 11.751538038253784\n",
      "Writing the final result to dev.p3.out...\n",
      "Finished Writing.\n",
      "Performing Viterbi for k best paths\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/estee/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in log\n",
      "/mnt/c/Users/estee/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in log\n",
      "/mnt/c/Users/estee/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Best Parallel Viterbi 106.15021395683289\n",
      "Writing the final result to dev.p4.out...\n",
      "Finished Writing.\n",
      "Performing sentiment analysis for data folder  AL\n",
      "There are 174948 observations, 3891 values of x is to be replaced \n",
      "Predicting labels\n",
      "0    杭 B-CITY\n",
      "1    州 I-CITY\n",
      "2    市 I-CITY\n",
      "Name: x, dtype: object\n",
      "Writing the final result to dev.p2.out...\n",
      "Finished writing\n",
      "Performing Viterbi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/estee/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n",
      "/mnt/c/Users/estee/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:80: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Viterbi and Backtrack 196.39462113380432\n",
      "Writing the final result to dev.p3.out...\n",
      "Finished Writing.\n",
      "Performing Viterbi for k best paths\n",
      "Time taken for K Best Parallel Viterbi 3604.882967710495\n",
      "Writing the final result to dev.p4.out...\n",
      "Finished Writing.\n",
      "Performing sentiment analysis for data folder  SG\n",
      "There are 336220 observations, 47404 values of x is to be replaced \n",
      "Predicting labels\n",
      "0       Tour B-positive\n",
      "1    Scotland B-neutral\n",
      "2           followers O\n",
      "Name: x, dtype: object\n",
      "Writing the final result to dev.p2.out...\n",
      "Finished writing\n",
      "Performing Viterbi\n",
      "Time taken for Viterbi and Backtrack 7.978028774261475\n",
      "Writing the final result to dev.p3.out...\n",
      "Finished Writing.\n",
      "Performing Viterbi for k best paths\n",
      "Time taken for K Best Parallel Viterbi 82.87734913825989\n",
      "Writing the final result to dev.p4.out...\n",
      "Finished Writing.\n",
      "Performing sentiment analysis for data folder  EN\n",
      "There are 181627 observations, 15287 values of x is to be replaced \n",
      "Predicting labels\n",
      "0     #UNK# B-UCP\n",
      "1        has B-VP\n",
      "2    close B-ADJP\n",
      "Name: x, dtype: object\n",
      "Writing the final result to dev.p2.out...\n",
      "Finished writing\n",
      "Performing Viterbi\n",
      "Time taken for Viterbi and Backtrack 28.94420576095581\n",
      "Writing the final result to dev.p3.out...\n",
      "Finished Writing.\n",
      "Performing Viterbi for k best paths\n",
      "Time taken for K Best Parallel Viterbi 506.09115648269653\n",
      "Writing the final result to dev.p4.out...\n",
      "Finished Writing.\n"
     ]
    }
   ],
   "source": [
    "from csv import QUOTE_NONE\n",
    "import time\n",
    "\n",
    "k = 3\n",
    "replaceWord = \"#UNK#\"\n",
    "data_folders = [\"CN\",\"AL\", \"SG\", \"EN\"]\n",
    "for x in data_folders:\n",
    "    print(\"Performing sentiment analysis for data folder \", x)\n",
    "    train_data = \"./data/{}/train\".format(x)\n",
    "    test_data = \"./data/{}/dev.in\".format(x)\n",
    "    \n",
    "    \"\"\"## part2 ##\"\"\"\n",
    "    train_arr = pd.read_csv(train_data, sep='\\r\\\\n', index_col=False, engine=\"python\", encoding=\"UTF-8\").to_numpy()\n",
    "    emission_dict, valid_x, xy_dict = emissionPara(train_arr, k, replaceWord)\n",
    "\n",
    "    testdf = pd.read_csv(test_data, sep='\\r\\n', names=['x'],index_col=False,skip_blank_lines=False, engine=\"python\", encoding=\"UTF-8\")\n",
    "    testdf = predict(testdf, xy_dict, replaceWord)\n",
    "    print(testdf.head(3))\n",
    "    \n",
    "    print(\"Writing the final result to dev.p2.out...\")\n",
    "    testdf.to_csv('./output/{}/dev.p2.out'.format(x), header=False, index=False, na_rep=\"\", sep=\"\\n\", quoting=QUOTE_NONE)\n",
    "    print(\"Finished writing\")\n",
    "    \n",
    "    \"\"\"## part 3 ##\"\"\"\n",
    "    train_df = pd.read_csv(train_data, sep='/r/n' ,index_col=False, engine=\"python\", skip_blank_lines=False, encoding=\"UTF-8\").to_numpy()\n",
    "    transition_dic, subseq_count, y_count = transitionPara(train_df)\n",
    "    nodes = list(y_count.keys())\n",
    "    testdf_unprocess = pd.read_csv(test_data, sep='/n', delimiter=None, names=['x'],index_col=False,skip_blank_lines=False, engine=\"python\")\n",
    "    lines= sentenceList(testdf_unprocess)\n",
    "    \n",
    "    \n",
    "    log_array =[]\n",
    "    sequence_log=[]\n",
    "    \n",
    "    print(\"Performing Viterbi\")\n",
    "    start = time.time()\n",
    "    for i in range(len(lines)):\n",
    "        viterbioutput=viterbi(lines[i])\n",
    "        log_array.append(viterbioutput)\n",
    "        path = viterbi_backtrack(viterbioutput)\n",
    "        sequence_log.append(path)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Time taken for Viterbi and Backtrack\", end - start)\n",
    "    \n",
    "    result = finalresult(sequence_log,testdf_unprocess)\n",
    "    \n",
    "    print(\"Writing the final result to dev.p3.out...\")\n",
    "    f = open('./output/{}/dev.p3.out'.format(x) ,'w')\n",
    "    for word in result['result']:\n",
    "        if pd.isnull(word) == False:\n",
    "            f.write(word + '\\n')\n",
    "        else:\n",
    "            f.write(\"\" +\"\\n\")\n",
    "    f.close()\n",
    "    print(\"Finished Writing.\")\n",
    "\n",
    "\n",
    "    \"\"\"## part 4 ##\"\"\"\n",
    "    print(\"Performing Viterbi for k best paths\")\n",
    "    sequence_log=[]\n",
    "       \n",
    "    start = time.time()\n",
    "    k = 7\n",
    "    for i in range(len(lines)):\n",
    "        log_array.append(viterbioutput)\n",
    "        path, path_probs, scores, phi = kViterbiParallel(nodes, lines[i], k)\n",
    "        best_path_indexes = path[0]\n",
    "        best_path = [nodes[x] for x in best_path_indexes]\n",
    "        sequence_log.append(best_path)\n",
    "        \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Time taken for K Best Parallel Viterbi\", end - start)\n",
    "    \n",
    "    result = finalresult(sequence_log,testdf_unprocess)\n",
    "    \n",
    "    print(\"Writing the final result to dev.p4.out...\")\n",
    "    f = open('./output/{}/dev.p4.out'.format(x) ,'w')\n",
    "    for word in result['result']:\n",
    "        if pd.isnull(word) == False:\n",
    "            f.write(word + '\\n')\n",
    "        else:\n",
    "            f.write(\"\" +\"\\n\")\n",
    "    f.close()\n",
    "    print(\"Finished Writing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script to get the evalscript results for part 2-4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------- Evaluation Results for **AL**------------\n",
      "*Part 2*\n",
      "\n",
      "#Entity in gold data: 8408\n",
      "#Entity in prediction: 19587\n",
      "\n",
      "#Correct Entity : 1780\n",
      "Entity  precision: 0.0909\n",
      "Entity  recall: 0.2117\n",
      "Entity  F: 0.1272\n",
      "\n",
      "#Correct Sentiment : 1053\n",
      "Sentiment  precision: 0.0538\n",
      "Sentiment  recall: 0.1252\n",
      "Sentiment  F: 0.0752\n",
      "\n",
      "*Part3*\n",
      "\n",
      "#Entity in gold data: 8408\n",
      "#Entity in prediction: 8467\n",
      "\n",
      "#Correct Entity : 3300\n",
      "Entity  precision: 0.3897\n",
      "Entity  recall: 0.3925\n",
      "Entity  F: 0.3911\n",
      "\n",
      "#Correct Sentiment : 2449\n",
      "Sentiment  precision: 0.2892\n",
      "Sentiment  recall: 0.2913\n",
      "Sentiment  F: 0.2903\n",
      "\n",
      "*Part4 - Only printing the evalScript results for best path*\n",
      "\n",
      "#Entity in gold data: 8408\n",
      "#Entity in prediction: 7294\n",
      "\n",
      "#Correct Entity : 624\n",
      "Entity  precision: 0.0855\n",
      "Entity  recall: 0.0742\n",
      "Entity  F: 0.0795\n",
      "\n",
      "#Correct Sentiment : 51\n",
      "Sentiment  precision: 0.0070\n",
      "Sentiment  recall: 0.0061\n",
      "Sentiment  F: 0.0065\n",
      "\n",
      " ----------- Evaluation Results for **EN**------------\n",
      "*Part 2*\n",
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 19883\n",
      "\n",
      "#Correct Entity : 8832\n",
      "Entity  precision: 0.4442\n",
      "Entity  recall: 0.6702\n",
      "Entity  F: 0.5343\n",
      "\n",
      "#Correct Sentiment : 6950\n",
      "Sentiment  precision: 0.3495\n",
      "Sentiment  recall: 0.5274\n",
      "Sentiment  F: 0.4204\n",
      "\n",
      "*Part3*\n",
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 12726\n",
      "\n",
      "#Correct Entity : 10692\n",
      "Entity  precision: 0.8402\n",
      "Entity  recall: 0.8113\n",
      "Entity  F: 0.8255\n",
      "\n",
      "#Correct Sentiment : 10281\n",
      "Sentiment  precision: 0.8079\n",
      "Sentiment  recall: 0.7801\n",
      "Sentiment  F: 0.7937\n",
      "\n",
      "*Part4 - Only printing the evalScript results for best path*\n",
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 19437\n",
      "\n",
      "#Correct Entity : 7285\n",
      "Entity  precision: 0.3748\n",
      "Entity  recall: 0.5528\n",
      "Entity  F: 0.4467\n",
      "\n",
      "#Correct Sentiment : 5199\n",
      "Sentiment  precision: 0.2675\n",
      "Sentiment  recall: 0.3945\n",
      "Sentiment  F: 0.3188\n",
      "\n",
      " ----------- Evaluation Results for **CN**------------\n",
      "*Part 2*\n",
      "\n",
      "#Entity in gold data: 1478\n",
      "#Entity in prediction: 9373\n",
      "\n",
      "#Correct Entity : 765\n",
      "Entity  precision: 0.0816\n",
      "Entity  recall: 0.5176\n",
      "Entity  F: 0.1410\n",
      "\n",
      "#Correct Sentiment : 285\n",
      "Sentiment  precision: 0.0304\n",
      "Sentiment  recall: 0.1928\n",
      "Sentiment  F: 0.0525\n",
      "\n",
      "*Part3*\n",
      "\n",
      "#Entity in gold data: 1478\n",
      "#Entity in prediction: 700\n",
      "\n",
      "#Correct Entity : 305\n",
      "Entity  precision: 0.4357\n",
      "Entity  recall: 0.2064\n",
      "Entity  F: 0.2801\n",
      "\n",
      "#Correct Sentiment : 208\n",
      "Sentiment  precision: 0.2971\n",
      "Sentiment  recall: 0.1407\n",
      "Sentiment  F: 0.1910\n",
      "\n",
      "*Part4 - Only printing the evalScript results for best path*\n",
      "\n",
      "#Entity in gold data: 1478\n",
      "#Entity in prediction: 1435\n",
      "\n",
      "#Correct Entity : 160\n",
      "Entity  precision: 0.1115\n",
      "Entity  recall: 0.1083\n",
      "Entity  F: 0.1099\n",
      "\n",
      "#Correct Sentiment : 101\n",
      "Sentiment  precision: 0.0704\n",
      "Sentiment  recall: 0.0683\n",
      "Sentiment  F: 0.0693\n",
      "\n",
      " ----------- Evaluation Results for **SG**------------\n",
      "*Part 2*\n",
      "\n",
      "#Entity in gold data: 4537\n",
      "#Entity in prediction: 20180\n",
      "\n",
      "#Correct Entity : 942\n",
      "Entity  precision: 0.0467\n",
      "Entity  recall: 0.2076\n",
      "Entity  F: 0.0762\n",
      "\n",
      "#Correct Sentiment : 276\n",
      "Sentiment  precision: 0.0137\n",
      "Sentiment  recall: 0.0608\n",
      "Sentiment  F: 0.0223\n",
      "\n",
      "*Part3*\n",
      "\n",
      "#Entity in gold data: 4537\n",
      "#Entity in prediction: 2547\n",
      "\n",
      "#Correct Entity : 138\n",
      "Entity  precision: 0.0542\n",
      "Entity  recall: 0.0304\n",
      "Entity  F: 0.0390\n",
      "\n",
      "#Correct Sentiment : 81\n",
      "Sentiment  precision: 0.0318\n",
      "Sentiment  recall: 0.0179\n",
      "Sentiment  F: 0.0229\n",
      "\n",
      "*Part4 - Only printing the evalScript results for best path*\n",
      "\n",
      "#Entity in gold data: 4537\n",
      "#Entity in prediction: 5857\n",
      "\n",
      "#Correct Entity : 240\n",
      "Entity  precision: 0.0410\n",
      "Entity  recall: 0.0529\n",
      "Entity  F: 0.0462\n",
      "\n",
      "#Correct Sentiment : 109\n",
      "Sentiment  precision: 0.0186\n",
      "Sentiment  recall: 0.0240\n",
      "Sentiment  F: 0.0210\n"
     ]
    }
   ],
   "source": [
    "!python3 evalResult.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
