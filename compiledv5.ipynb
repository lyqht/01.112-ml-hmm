{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.112 Machine Learning Design Project\n",
    "\n",
    "## About the Project\n",
    "\n",
    "We have 4 datasets in the `/data` folder. For each dataset, there is: \n",
    "- a labelled training set train, \n",
    "- an unlabelled development set `dev.in`\n",
    "- a labelled development set `dev.out` \n",
    "\n",
    "The labelled data has the format of: `token` `\\t` `tag`\n",
    "- one token per line\n",
    "- token and tag separated by tab \n",
    "- single empty lines that separates sentences\n",
    "\n",
    "For the labels, they are slightly different for different datasets.\n",
    "- SG, CN (Entity):\n",
    "    - B-*: Beginning of entity\n",
    "    - I-*: Inside of entity\n",
    "    - O: Outside of any entity\n",
    "- EN, AL (Phrase):\n",
    "    - B-VP: Beginning of Verb Phrase\n",
    "    - I-VP: Inside of Verb Phrase\n",
    "    - *-NP: Noun Phrase\n",
    "    - *PP: Propositional Phrase\n",
    "    - O: Outside of any phrase\n",
    "\n",
    "*Goal*: Build sequence labelling systems from training data (x) and use it to predict tag sequences for new sentences (y).\n",
    "\n",
    "## Team members \n",
    "- Andri Setiawan Susanto\n",
    "- Eldon Lim \n",
    "- Tey Siew Wen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Already completed individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Write a function that estimates the emission parameters from the training set using MLE (maximum likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emissionPara(xy,y):\n",
    "    e_x_y= xy/y\n",
    "    \n",
    "    return e_x_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\n",
    "\n",
    "1. Make a modified training set by replacing those words that appear $<k$ times in the training set with a special word token `#UNK#` before training.\n",
    "2. During testing phase, ifaworddoesnot appear in the modified training set, we also replace that wordwith `#UNK#`.\n",
    "3. Compute Emission Paramters with the function in (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "replaceWord = \"#UNK#\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_columns(df_column):\n",
    "    new = df_column.str.split(\" \", n=1, expand=True)\n",
    "    return new[0], new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training(data):\n",
    "    global replaceWord\n",
    "    \n",
    "      \n",
    "    x_dic = {}\n",
    "    \n",
    "    # dropping null value columns e.g. index_col to avoid errors \n",
    "    df= pd.read_csv(data, sep='/n', delimiter=None, names=['original'],index_col=False, engine=\"python\")\n",
    "    \n",
    "    # new data frame with split value columns \n",
    "    df[\"x\"], df[\"y\"] = split_into_columns(df[\"original\"])\n",
    "    \n",
    "    return df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueCount(df,k,replaceWord):\n",
    "    start = time.process_time() \n",
    "    x_dic={}\n",
    "     # df display: record x value and replace y values with replaceWord when necessary, in respective dictionaries\n",
    "    uniqueX, uniqueCountX= np.unique(df['x'].astype(str),return_counts=True)\n",
    "    for i in range(len(uniqueX)):\n",
    "        x_dic[uniqueX[i]] = uniqueCountX[i]\n",
    "\n",
    "    for i, text in enumerate(df['x']):\n",
    "        if x_dic[text] < k:\n",
    "            df['x'][i] = replaceWord\n",
    "            df['original'][i]=df['original'][i].replace(text,replaceWord, 1)\n",
    "    \n",
    "    y_dic={}\n",
    "    \n",
    "    uniqueY, uniqueCountY= np.unique(df['y'].astype(str),return_counts=True)\n",
    "    for i in range(len(uniqueY)):\n",
    "        y_dic[uniqueY[i]] = uniqueCountY[i]\n",
    "        \n",
    "    xy_dic = {}\n",
    "    df1= df.copy()\n",
    "    \n",
    "    # Get a tuple of unique values & their count from a numpy array\n",
    "    df1.dropna(inplace = True) \n",
    "    uniqueXY, uniqueCountXY= np.unique(df1['original'].astype(str),return_counts=True)\n",
    "\n",
    "    for i in range(len(uniqueXY)):\n",
    "        xy_dic[uniqueXY[i]] = uniqueCountXY[i]\n",
    "    # print('Unique Values : ', uniqueValues)\n",
    "    \n",
    "    # print('Count of Unique Values : ', uniqueCount)\n",
    "    dft = pd.DataFrame([uniqueXY,uniqueCountXY]).T\n",
    "    dft=dft.rename({0:'x_y',1:'count_x_y'},axis='columns')\n",
    "    \n",
    "    dft['count_y']=0\n",
    "    for i,text in enumerate(dft['x_y']):\n",
    "        data = text.split(\" \")\n",
    "        dft['count_y'][i]=y_dic[data[1]]\n",
    "    print(\"Time taken for train data: \", time.process_time() - start)\n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emissionCalcu(df):\n",
    "    emission_dict={}\n",
    "    df['emission']=emissionPara(df['count_x_y'], df['count_y'])\n",
    "    for i in range(df.shape[0]):\n",
    "        emission_dict[df['x_y'][i]]= df['emission'][i]\n",
    "    return df,emission_dict\n",
    "\n",
    "def xyPrediction(dft):\n",
    "    # new data frame with split value columns \n",
    "    dft1 = dft.copy()\n",
    "    dft1[\"x\"], dft1[\"y\"] = split_into_columns(dft1[\"x_y\"])\n",
    "    \n",
    "    xy_pred_dic = {}\n",
    "\n",
    "    for word in dft1['x']:\n",
    "        index = pd.Series.idxmax((dft1.loc[dft1['x'] == word]['emission']).astype(float))\n",
    "        xy_pred_dic[word]=dft1['y'][index] \n",
    "    \n",
    "    return xy_pred_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(data,k):\n",
    "    global replaceWord\n",
    "    \n",
    "    start = time.process_time()   \n",
    "\n",
    "    testdf1= pd.read_csv(data, sep='/n', delimiter=None, names=['original'],index_col=False, engine=\"python\")\n",
    "    testdf= pd.read_csv(data, sep='/n', delimiter=None, names=['original'],index_col=False,skip_blank_lines=False, engine=\"python\")\n",
    "\n",
    "    x_dic = {}\n",
    "\n",
    "    uniqueX, uniqueCountX= np.unique(testdf1['original'].astype(str),return_counts=True)\n",
    "    for i in range(len(uniqueX)):\n",
    "        x_dic[uniqueX[i]] = uniqueCountX[i]\n",
    "\n",
    "    testdf['modified']=''\n",
    "#     print(testdf)\n",
    "    for i, text in enumerate(testdf['original']):\n",
    "    #         df['x'][i] = replaceWord\n",
    "        try:\n",
    "            if text not in xy_pred_dic:\n",
    "            \n",
    "                testdf['modified'][i]=testdf['original'][i].replace(text,replaceWord)\n",
    "            else:\n",
    "                testdf['modified'][i]=testdf['original'][i]\n",
    "        except:\n",
    "            continue\n",
    "    testdf['predict_label']=''\n",
    "    for index, word in enumerate(testdf['modified']):\n",
    "#     print(word)\n",
    "        try:\n",
    "            testdf['predict_label'][index]= xy_pred_dic[word]\n",
    "        except:\n",
    "            continue\n",
    "    print(\"Time taken for test data: \",time.process_time() - start)\n",
    "    return testdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the four datasets EN, AL, CN, and SG, learn these parameters with `train`, and evaluate your\n",
    "system on the development set `dev.in` for each of the dataset. Write your output to `dev.p2.out`\n",
    "for the four datasets respectively. Compare your outputs and the gold-standard outputs in `dev.out`\n",
    "and report the precision, recall and F scores of such a baseline system for each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that estimates the transition parameters from the training set using MLE (maximum likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(\"./data/EN/train\", sep='/n', delimiter=None, names=['original'],index_col=False, engine=\"python\", skip_blank_lines=False)\n",
    "# test.replace(np.nan, None, inplace=True)\n",
    "# test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def transitionPara(data):\n",
    "    train_data_blank=pd.read_csv(data, sep='/n', delimiter=None, names=['original'],index_col=False, engine=\"python\", skip_blank_lines=False)\n",
    "    x, y = split_into_columns(train_data_blank[\"original\"])\n",
    "    xy_dic = dict(zip(x, y))\n",
    "    \n",
    "    # Get bottom count (Count(yi))\n",
    "    y_count = Counter(y)\n",
    "    \n",
    "    # Get top count (Count(yi-1, yi))\n",
    "    subseq_count = defaultdict(int)\n",
    "    for i in range(len(y)-1):    \n",
    "        y1 = y[i]\n",
    "        y2 = y[i+1]\n",
    "        \n",
    "        if i == 0:\n",
    "            subseq_count[(\"START\", y1)] +=1\n",
    "            y_count[\"START\"] +=1\n",
    "        if pd.isna(y1):\n",
    "            subseq_count[(\"START\", y2)] +=1\n",
    "            y_count[\"START\"] +=1\n",
    "        elif i == len(y)-1 or pd.isna(y2):\n",
    "            subseq_count[(y1, \"END\")] +=1\n",
    "            y_count[\"END\"] +=1\n",
    "        else:\n",
    "            subseq_count[y1,y2] += 1\n",
    "    \n",
    "    # Calculation of transition params\n",
    "    result = np.empty(len(y)+2)\n",
    "    transition_dict = {}\n",
    "    \n",
    "    for k,v in subseq_count.items():\n",
    "        y1 = k[0]\n",
    "        y2 = k[1]\n",
    "        transition_dict[y1,y2] = subseq_count[y1,y2] / y_count[y2]\n",
    "     \n",
    "    return transition_dict, subseq_count, y_count\n",
    "\n",
    "# transition_dic, subseq_count, y_count = transitionPara(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def viterbi(unique_word_list):\n",
    "    #This is for the starting for viterbi\n",
    "    store=[]   #store = the storage for scores for all the nodes.\n",
    "    scorelist=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #This is for the start\n",
    "    for i in range(len(nodes)):\n",
    "        emission_score = emission(nodes[i],unique_word_list[0])\n",
    "        transition_score = transition(\"START\",nodes[i])\n",
    "        score_at_start = np.log(emission_score)+np.log(transition_score)\n",
    "        store.append(score_at_start)    \n",
    "        \n",
    "    scorelist.append(store)\n",
    "    store=[]\n",
    "    score_per_node=[]\n",
    "    \n",
    "    #This is for the middle portion for viterbi\n",
    "    if len(unique_word_list) > 1:\n",
    "        for j in range(len(unique_word_list) - 1): #for the whole length in sentence\n",
    "            for k in range(len(nodes)): # for each node\n",
    "                #score per node = prevnode*emission*transition\n",
    "                \n",
    "                for l in range(len(nodes)): # l = iterate thru previous node, k= iterate thru current node, j= iterate thru sentence\n",
    "                    # This is to calculate the current node scores.\n",
    "                    prev_node = scorelist[j][l]\n",
    "                    emission_score = emission(nodes[k],unique_word_list[j+1])\n",
    "                    curr_node = nodes[k]\n",
    "                    score_per_node.append(prev_node+np.log(emission_score)+np.log(transition(nodes[l],curr_node)))\n",
    "                    \n",
    "                store.append(max(score_per_node)) # found max path\n",
    "                \n",
    "#                 max_score = max(score_per_node)\n",
    "#                 max_index = np.argmax(score_per_node)                \n",
    "#                 label_max = labels[max_index]\n",
    "                score_per_node=[]\n",
    "            \n",
    "            #print(store)\n",
    "            scorelist.append(store) # store the scores for nodes\n",
    "            store=[]\n",
    "\n",
    "                      \n",
    "        score_at_stop=[]\n",
    "        #This is for the stop for viterbi\n",
    "        for m in range(len(nodes)):\n",
    "            score_at_stop.append(np.log(transition(nodes[m],\"END\"))+ (scorelist[len(unique_word_list)-1][m])) #at stop.\n",
    "        scorelist.append(max(score_at_stop))\n",
    "     \n",
    "    return scorelist\n",
    "\n",
    "    \n",
    "def emission(node,word):\n",
    "    global emission_dict\n",
    "    # Will use this to search the emission score for the given word\n",
    "    #emission_dict={\"Athe\":0.9,\"Bthe\":0.1,\"Adog\":0.1,\"Bdog\":0.9,\"Astop\":0}   # takes out from dictionary\n",
    "    pair = word+\" \"+node\n",
    "    if pair not in emission_dict.keys():\n",
    "        replaced_text = \"#UNK# \"+ node\n",
    "        if replaced_text in emission_dict.keys():\n",
    "            score = 0.01*emission_dict[replaced_text]\n",
    "        else:\n",
    "            score = 0 # our train data does not have this node\n",
    "    else:\n",
    "        score = emission_dict[pair]\n",
    "    \n",
    "    return score\n",
    "\n",
    "def transition(x1,x2):\n",
    "    global transition_dic\n",
    "    #will use this to search the transition from x1 to x2\n",
    "    pair = x1,x2\n",
    "    if pair not in transition_dic.keys():\n",
    "        score = 0\n",
    "    else:\n",
    "        score = transition_dic[x1,x2]\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backtrack(scorelist):\n",
    "    ####### back tracking for viterbi\n",
    "    # node value*transition = array, then find max, then find position. use position for next step.\n",
    "    #np.argmax returns index of max in the element.\n",
    "    # The final score on the score list is for end\n",
    "    scorelist = scorelist[::-1] #reverse the score list so easier to calculate.\n",
    "    node_holder=[]\n",
    "    path = []\n",
    "    max_node_index=0\n",
    "    length_of_scorelist=len(scorelist)\n",
    "    length_of_nodes=len(nodes)\n",
    "\n",
    "    if (length_of_scorelist == 1):\n",
    "        for k in range (length_of_nodes):\n",
    "            calculate_max_node = (scorelist[0][k]) + np.log(transition(nodes[k],\"END\"))\n",
    "            node_holder.append(calculate_max_node)\n",
    "        path.append(nodes[np.argmax(node_holder)])\n",
    "        node_holder=[]\n",
    "        return(path[::-1])\n",
    "\n",
    "    for i in range (1,length_of_scorelist): # for length of sentence\n",
    "\n",
    "        for j in range(length_of_nodes): #for each node\n",
    "            #each node*own path, find max\n",
    "            if (i==1):\n",
    "                calculate_max_node = (scorelist[i][j]) + np.log(transition(nodes[j],\"END\"))\n",
    "                node_holder.append(calculate_max_node)\n",
    "                #print(np.exp(calculate_max_node))\n",
    "            else:\n",
    "                \n",
    "                calculate_max_node = (scorelist[i][j]) + np.log(transition(nodes[j],nodes[max_node_index]))#\n",
    "                node_holder.append(calculate_max_node)\n",
    "        \n",
    "        max_node_index=(np.argmax(node_holder))\n",
    "        path.append(nodes[np.argmax(node_holder)])\n",
    "        node_holder=[]\n",
    "        \n",
    "\n",
    "    return(path[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_blank_row(data):\n",
    "    start = time.process_time()   \n",
    "    \n",
    "    df= pd.read_csv(data, sep='/n', delimiter=None, names=['original'],index_col=False,engine=\"python\",skip_blank_lines=False)\n",
    "    # dropping null value columns to avoid errors \n",
    "    \n",
    "    # new data frame with split value columns \n",
    "    df[\"x\"], df[\"y\"] = split_into_columns(df[\"original\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceList(data):\n",
    "    lines=[]\n",
    "    line=[]\n",
    "    x= data\n",
    "    for label in x['x']:\n",
    "        if pd.isnull(label)==False:\n",
    "            line.append(label)\n",
    "        else:\n",
    "    #         line += ' stop'\n",
    "            lines.append(line)\n",
    "            line = []\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalresult(sequence_log,predata_blank):\n",
    "    dataframe = []\n",
    "    count=0\n",
    "    for i in range(len(sequence_log)):\n",
    "        for text in sequence_log[i]:\n",
    "            dataframe.append(text)\n",
    "            count+=1\n",
    "        dataframe.append(\"\")\n",
    "    dftest=pd.DataFrame(dataframe)\n",
    "    final = pd.DataFrame()\n",
    "    final['result'] = predata_blank['x'] + \" \" +dftest[0]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing sentiment analysis for data folder  EN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for train data:  299.703125\n",
      "Time taken for test data:  9.46875\n",
      "Writing the final result to dev.out...\n",
      "                 x\n",
      "0              HBO\n",
      "1              has\n",
      "2            close\n",
      "3               to\n",
      "4               24\n",
      "5          million\n",
      "6      subscribers\n",
      "7               to\n",
      "8              its\n",
      "9              HBO\n",
      "10             and\n",
      "11         Cinemax\n",
      "12        networks\n",
      "13               ,\n",
      "14           while\n",
      "15        Showtime\n",
      "16             and\n",
      "17             its\n",
      "18          sister\n",
      "19         service\n",
      "20               ,\n",
      "21             The\n",
      "22           Movie\n",
      "23         Channel\n",
      "24               ,\n",
      "25            have\n",
      "26            only\n",
      "27           about\n",
      "28              10\n",
      "29         million\n",
      "...            ...\n",
      "27195           is\n",
      "27196          due\n",
      "27197         only\n",
      "27198       partly\n",
      "27199           to\n",
      "27200          the\n",
      "27201    austerity\n",
      "27202      program\n",
      "27203     launched\n",
      "27204           in\n",
      "27205    September\n",
      "27206         1988\n",
      "27207           to\n",
      "27208         cool\n",
      "27209           an\n",
      "27210   overheated\n",
      "27211      economy\n",
      "27212          and\n",
      "27213         tame\n",
      "27214    inflation\n",
      "27215            .\n",
      "27216          NaN\n",
      "27217          The\n",
      "27218          GOP\n",
      "27219     doubters\n",
      "27220         were\n",
      "27221           in\n",
      "27222     Congress\n",
      "27223            .\n",
      "27224          NaN\n",
      "\n",
      "[27225 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 result\n",
      "0            HBO B-ADVP\n",
      "1              has B-VP\n",
      "2            close I-VP\n",
      "3               to B-PP\n",
      "4               24 B-NP\n",
      "5          million I-NP\n",
      "6      subscribers B-VP\n",
      "7               to I-VP\n",
      "8              its B-NP\n",
      "9              HBO B-VP\n",
      "10                and O\n",
      "11         Cinemax B-NP\n",
      "12        networks I-NP\n",
      "13                  , O\n",
      "14         while B-SBAR\n",
      "15        Showtime B-NP\n",
      "16                and O\n",
      "17             its B-NP\n",
      "18          sister I-NP\n",
      "19         service I-NP\n",
      "20                  , O\n",
      "21             The B-NP\n",
      "22           Movie I-NP\n",
      "23         Channel I-NP\n",
      "24                  , O\n",
      "25            have B-VP\n",
      "26            only I-VP\n",
      "27           about B-PP\n",
      "28              10 B-NP\n",
      "29         million I-NP\n",
      "...                 ...\n",
      "27195           is B-VP\n",
      "27196        due B-ADJP\n",
      "27197       only I-ADJP\n",
      "27198     partly I-ADJP\n",
      "27199           to B-PP\n",
      "27200          the B-NP\n",
      "27201    austerity I-NP\n",
      "27202      program I-NP\n",
      "27203     launched I-NP\n",
      "27204           in B-PP\n",
      "27205    September B-NP\n",
      "27206         1988 I-NP\n",
      "27207           to B-VP\n",
      "27208         cool I-VP\n",
      "27209           an B-NP\n",
      "27210   overheated I-NP\n",
      "27211      economy I-NP\n",
      "27212             and O\n",
      "27213         tame B-NP\n",
      "27214    inflation I-NP\n",
      "27215               . O\n",
      "27216               NaN\n",
      "27217          The B-NP\n",
      "27218          GOP I-NP\n",
      "27219     doubters I-NP\n",
      "27220         were B-VP\n",
      "27221           in B-PP\n",
      "27222     Congress B-NP\n",
      "27223               . O\n",
      "27224               NaN\n",
      "\n",
      "[27225 rows x 1 columns]\n",
      "Writing the final result to dev.p3.out...\n"
     ]
    }
   ],
   "source": [
    "data_folders = [\"AL\", \"EN\",\"CN\",\"SG\"]\n",
    "for x in [\"EN\"]:\n",
    "    print(\"Performing sentiment analysis for data folder \", x)\n",
    "    train_data = \"./data/{}/train\".format(x)\n",
    "    test_data = \"./data/{}/dev.in\".format(x)\n",
    "    test_result = \"./data/{}/dev.out\".format(x)\n",
    "    \n",
    "    predata = preprocess_training(train_data)\n",
    "    countData=uniqueCount(predata,k,replaceWord)\n",
    "    emissiondf = emissionCalcu(countData)\n",
    "    emission_dict = emissiondf[1]\n",
    "    xy_pred_dic = xyPrediction(emissiondf[0])\n",
    "    testdf_unprocess = pd.read_csv(test_data, sep='/n', delimiter=None, names=['x'],index_col=False,skip_blank_lines=False, engine=\"python\")\n",
    "    testdf = preprocess_test(test_data,k)\n",
    "    \n",
    "    testresultdf = pd.read_csv(test_result, sep='/n', delimiter=None, names=['original'],index_col=False, engine=\"python\")\n",
    "    new = testresultdf[\"original\"].str.split(\" \", n=1,expand=True) \n",
    "\n",
    "    # making separate first name column from new data frame \n",
    "    testresultdf[\"x\"]= new[0] \n",
    "\n",
    "    # making separate last name column from new data frame \n",
    "    testresultdf[\"y\"]= new[1]\n",
    "    final = pd.DataFrame()\n",
    "    \n",
    "    final['result'] = testdf['modified'] + ' ' + testdf['predict_label']\n",
    "#     print(final.head(3))\n",
    "    \n",
    "    print(\"Writing the final result to dev.out...\")\n",
    "    f = open('./output/{}/dev.p2.out'.format(x) ,'w')\n",
    "    for word in final['result']:\n",
    "        f.write(word + '\\n')\n",
    "    f.close()\n",
    "    \n",
    "##############################PART 3########################################################\n",
    "    transition_dic, subseq_count, y_count = transitionPara(train_data)\n",
    "    predata_blank=preprocess_training_blank_row(train_data)\n",
    "    node = np.unique(predata['y'].astype(str))\n",
    "    print(testdf_unprocess)\n",
    "    lines= sentenceList(testdf_unprocess)\n",
    "    \n",
    "    nodes = node\n",
    "    log_array =[]\n",
    "    sequence_log=[]\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        viterbioutput=viterbi(lines[i])\n",
    "        log_array.append(viterbioutput)\n",
    "        sequence_log.append(viterbi_backtrack(viterbioutput))\n",
    "    \n",
    "    result = finalresult(sequence_log,testdf_unprocess)\n",
    "    print(result)\n",
    "    \n",
    "    print(\"Writing the final result to dev.p3.out...\")\n",
    "#     f = open('./dev.p3.out'.format(x) ,'w')\n",
    "    f = open('./output/{}/dev.p3.out'.format(x) ,'w')\n",
    "    for word in result['result']:\n",
    "        if pd.isnull(word) == False:\n",
    "            f.write(word + '\\n')\n",
    "        else:\n",
    "            f.write(\"\" +\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
